{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcvqQGSbyKYJ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "wez-pskPyNDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QZ2UIwAvyPKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive/NLP Project/final_factuality_model\"))"
      ],
      "metadata": {
        "id": "SbjV-Q9syQ6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the path where your model is saved\n",
        "model_path = \"/content/drive/MyDrive/NLP Project/final_factuality_model\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "HqtWzsPGyTki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the complete dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP Project/preprocessed_dataset_final1.csv\")\n",
        "\n",
        "# Split into 80% train and 20% test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the size of train and test splits\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "Nhzeu6YFyVDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['processed_claims'] = test_df['processed_claims'].astype(str)\n",
        "test_df['processed_evidence'] = test_df['processed_evidence'].astype(str)"
      ],
      "metadata": {
        "id": "toRQ0T55yWq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_claim_evidence(claim, evidence, threshold=0.5):\n",
        "    inputs = tokenizer(claim, evidence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    predicted_class = 1 if probabilities[0][1] > threshold else 0\n",
        "    return predicted_class"
      ],
      "metadata": {
        "id": "3Veiks3ZyYf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['predicted_label'] = test_df.apply(lambda row: predict_claim_evidence(row['processed_claims'], row['processed_evidence']), axis=1)"
      ],
      "metadata": {
        "id": "qXWbMG81yaFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Calculate metrics without displaying them\n",
        "true_labels = test_df['label'].tolist()\n",
        "predicted_labels = test_df['predicted_label'].tolist()"
      ],
      "metadata": {
        "id": "UWOdhV-YybzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='binary')"
      ],
      "metadata": {
        "id": "-Yaf1DDnydf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_metrics = {\n",
        "    'accuracy': accuracy,\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1': f1\n",
        "}"
      ],
      "metadata": {
        "id": "abIr5xA4yfkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_comparison_bar_chart():\n",
        "    labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "    values = [performance_metrics['accuracy'], performance_metrics['precision'], performance_metrics['recall'], performance_metrics['f1']]\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    bars = plt.bar(labels, values, color=['blue', 'green', 'orange', 'red'])\n",
        "    plt.title('Model Performance Metrics')\n",
        "    plt.ylim(0, 1)  # Values are between 0 and 1\n",
        "    plt.ylabel('Score')\n",
        "\n",
        "    # Add values on top of the bars\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.02, round(yval, 4), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "K0iqwX6zyhft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_comparison_bar_chart()"
      ],
      "metadata": {
        "id": "ICrrqPYtyjNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "B8hmDOF_ylPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_samples = len(true_labels)\n",
        "tp = int(recall * (accuracy * total_samples))\n",
        "fp = int((tp / precision) - tp)\n",
        "fn = int((tp / recall) - tp)\n",
        "tn = total_samples - (tp + fp + fn)\n",
        "\n",
        "# Confusion matrix array\n",
        "conf_matrix = np.array([[tn, fp], [fn, tp]])\n",
        "\n",
        "# Plot confusion matrix\n",
        "def plot_confusion_matrix(cm):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot the confusion matrix\n",
        "plot_confusion_matrix(conf_matrix)"
      ],
      "metadata": {
        "id": "Ahqp2U2Vym-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}